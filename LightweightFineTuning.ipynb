{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: distilbert-base-uncased\n",
    "* Evaluation approach: I am evaluating classification accuracy by comparing predicted and actual labels.\n",
    "* Fine-tuning dataset: rkf2778/amazon_reviews_mobile_electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cody\\Desktop\\projects\\lwft_hf_model\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "my_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "# Criteria #1. Load a pretrained HF model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    my_model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(my_model_name)\n",
    "\n",
    "\n",
    "# Dataset to be used: Amazon reviews of mobile electronics\n",
    "my_dataset_name = \"rkf2778/amazon_reviews_mobile_electronics\"\n",
    "splits = [\"test\", \"train\"]\n",
    "\n",
    "# Criteria #2a Load dataset\n",
    "datasets = {\n",
    "    split: dataset\n",
    "    for split, dataset in zip(splits, load_dataset(my_dataset_name, split=splits))\n",
    "}\n",
    "\n",
    "# Reduce the number of data for efficiency\n",
    "for split in splits:\n",
    "    datasets[split] = datasets[split].shuffle(seed=50).select(range(500))\n",
    "    \n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "test_dataset = datasets[\"test\"]\n",
    "    \n",
    "# Criteria 2b: Preprocess the dataset inlcuding the star_rating_label representing the sentiment\n",
    "def preprocess_review(batch):\n",
    "    tokenized = tokenizer(batch[\"review_body\"], padding=True, truncation=True, max_length=512)\n",
    "    tokenized[\"labels\"] = [label2id[label] for label in batch[\"star_rating_label\"]]\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_dataset.map(preprocess_review, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_review, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cody\\AppData\\Local\\Temp\\ipykernel_10896\\607905029.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 03:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7221406102180481,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_accuracy': 0.356,\n",
       " 'eval_runtime': 10.7939,\n",
       " 'eval_samples_per_second': 46.322,\n",
       " 'eval_steps_per_second': 5.837}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Accuracy calculation function and Trainer setup is from Udacity lesson 4.14\n",
    "def calculate_accuracy(predictions):\n",
    "    logits, labels = predictions\n",
    "    predicted_class = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": (predicted_class == labels).mean()}\n",
    "\n",
    "\n",
    "# Reference:  Udacity lession 4.14\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/original_model_sentiments_output\",\n",
    "    per_device_eval_batch_size=8\n",
    ")\n",
    "\n",
    "my_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create a trainer (referenced from Udacity lession 4.14)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=my_data_collator,\n",
    "    compute_metrics=calculate_accuracy\n",
    ")\n",
    "\n",
    "# Criteria #3: Evaluate the pretrained model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 67,694,596 || trainable%: 0.2178\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "peft_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    my_model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Reference: https://huggingface.co/docs/peft/package_reference/lora\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"], # Layers specific to distilbert\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Criteria #4: Create a PEFT model\n",
    "peft_model = get_peft_model(peft_base_model, peft_config)\n",
    "\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True  # LoRA parameters should be trainable\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cody\\Desktop\\projects\\lwft_hf_model\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cody\\AppData\\Local\\Temp\\ipykernel_10896\\2791392753.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  peft_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.503583</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.354992</td>\n",
       "      <td>0.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.382819</td>\n",
       "      <td>0.852000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Criteria #5: Train the PEFT model\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./data/peft_model_sentiments_output_2\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=my_data_collator,\n",
    "    compute_metrics=calculate_accuracy\n",
    ")\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "# Criteria #6: Save the PEFT model \n",
    "peft_model.save_pretrained(\"my_peft_lora_distilbert2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Cody\\AppData\\Local\\Temp\\ipykernel_10896\\2944450660.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tuned_trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model accuracy:\n",
      "   eval_loss  eval_model_preparation_time  eval_accuracy  eval_runtime  \\\n",
      "0   0.722141                          0.0          0.356       12.3916   \n",
      "\n",
      "   eval_samples_per_second  eval_steps_per_second  \n",
      "0                    40.35                  5.084  \n",
      "Fine-tuned Model accuracy:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_model_preparation_time  eval_accuracy  eval_runtime  \\\n",
      "0   0.354992                       0.0025          0.862       12.8495   \n",
      "\n",
      "   eval_samples_per_second  eval_steps_per_second  \n",
      "0                   38.912                  4.903  \n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Criteria #7: Load the saved PEFT model (https://huggingface.co/docs/peft/package_reference/auto_class)\n",
    "saved_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"my_peft_lora_distilbert2\")\n",
    "\n",
    "# Recreate the PEFT trainer from the saved model\n",
    "tuned_trainer = Trainer(\n",
    "    model=saved_peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=my_data_collator,\n",
    "    compute_metrics=calculate_accuracy\n",
    ")\n",
    "\n",
    "\n",
    "# Criteria #8: Compare and evaluate the fine tuned model\n",
    "print(\"Original Model accuracy:\")\n",
    "print(pd.DataFrame([trainer.evaluate()]))\n",
    "\n",
    "print(\"Fine-tuned Model accuracy:\")\n",
    "print(pd.DataFrame([tuned_trainer.evaluate()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
