{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: distilbert-base-uncased\n",
    "* Evaluation approach: I am evaluating classification accuracy by comparing predicted and actual labels.\n",
    "* Fine-tuning dataset: rkf2778/amazon_reviews_mobile_electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "my_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "# Criteria #1. Load a pretrained HF model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    my_model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(my_model_name)\n",
    "\n",
    "\n",
    "# Dataset to be used: Amazon reviews of mobile electronics\n",
    "my_dataset_name = \"rkf2778/amazon_reviews_mobile_electronics\"\n",
    "splits = [\"test\", \"train\"]\n",
    "\n",
    "# Criteria #2a Load dataset\n",
    "datasets = {\n",
    "    split: dataset\n",
    "    for split, dataset in zip(splits, load_dataset(my_dataset_name, split=splits))\n",
    "}\n",
    "\n",
    "# Reduce the number of data for efficiency\n",
    "for split in splits:\n",
    "    datasets[split] = datasets[split].shuffle(seed=50).select(range(500))\n",
    "    \n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "test_dataset = datasets[\"test\"]\n",
    "    \n",
    "# Criteria 2b: Preprocess the dataset inlcuding the star_rating_label representing the sentiment\n",
    "def preprocess_review(batch):\n",
    "    tokenized = tokenizer(batch[\"review_body\"], padding=True, truncation=True, max_length=512)\n",
    "    tokenized[\"labels\"] = [label2id[label] for label in batch[\"star_rating_label\"]]\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = train_dataset.map(preprocess_review, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_review, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669815</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/original_model_sentiments_output/checkpoint-63 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 05:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6698151230812073,\n",
       " 'eval_accuracy': 0.644,\n",
       " 'eval_runtime': 2.6141,\n",
       " 'eval_samples_per_second': 191.268,\n",
       " 'eval_steps_per_second': 24.1,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Accuracy calculation function and Trainer setup is from Udacity lesson 4.14\n",
    "def calculate_accuracy(predictions):\n",
    "    logits, labels = predictions\n",
    "    predicted_class = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predicted_class == labels).mean()}\n",
    "\n",
    "\n",
    "# Reference:  Udacity lession 4.14\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/original_model_sentiments_output\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "my_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create a trainer (referenced from Udacity lession 4.14)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=my_data_collator,\n",
    "    compute_metrics=calculate_accuracy\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Criteria #3: Evaluate the pretrained model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 67,694,596 || all params: 67,694,596 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "# Reference: https://huggingface.co/docs/peft/package_reference/lora\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"], # Layers specific to distilbert\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Criteria #4: Create a PEFT model\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "for param in peft_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 00:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.663358</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653831</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651145</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/peft_model_sentiments_output_2/checkpoint-63 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft_model_sentiments_output_2/checkpoint-126 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/peft_model_sentiments_output_2/checkpoint-189 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Criteria #5: Train the PEFT model\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./data/peft_model_sentiments_output_2\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=my_data_collator,\n",
    "    compute_metrics=calculate_accuracy\n",
    ")\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "# Criteria #6: Save the PEFT model \n",
    "peft_model.save_pretrained(\"my_peft_lora_distilbert2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model accuracy: {'eval_loss': 0.6979907155036926, 'eval_accuracy': 0.356, 'eval_runtime': 2.6654, 'eval_samples_per_second': 187.592, 'eval_steps_per_second': 23.637, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model accuracy: {'eval_loss': 0.6633461117744446, 'eval_accuracy': 0.644, 'eval_runtime': 2.7389, 'eval_samples_per_second': 182.555, 'eval_steps_per_second': 23.002}\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Criteria #7: Load the saved PEFT model (https://huggingface.co/docs/peft/package_reference/auto_class)\n",
    "saved_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"my_peft_lora_distilbert2\")\n",
    "\n",
    "# Recreate the PEFT trainer from the saved model\n",
    "tuned_trainer = Trainer(\n",
    "    model=saved_peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=my_data_collator,\n",
    "    compute_metrics=calculate_accuracy\n",
    ")\n",
    "\n",
    "\n",
    "# Criteria #8: Compare and evaluate the fine tuned model\n",
    "print(f\"Original Model accuracy: {trainer.evaluate()}\")\n",
    "print(f\"Fine-tuned Model accuracy: {tuned_trainer.evaluate()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
